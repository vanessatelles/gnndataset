{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "GCN_Cora.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTMdIdS_h2u9"
      },
      "source": [
        "# GCN node classification based on Cora dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HawIdURHh2vA"
      },
      "source": [
        "## SetUp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff9XPZ-rh2vB"
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "import os.path as osp\n",
        "import pickle\n",
        "import urllib\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Bm3fTLh2vK"
      },
      "source": [
        "## data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NWDFaJIh2vL"
      },
      "source": [
        "Data = namedtuple('Data', ['x', 'y', 'adjacency',\n",
        "                           'train_mask', 'val_mask', 'test_mask'])\n",
        "\n",
        "\n",
        "def tensor_from_numpy(x, device):\n",
        "    return torch.from_numpy(x).to(device)\n",
        "\n",
        "\n",
        "class CoraData(object):\n",
        "    download_url = \"https://raw.githubusercontent.com/kimiyoung/planetoid/master/data\"\n",
        "    filenames = [\"ind.cora.{}\".format(name) for name in\n",
        "                 ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']]\n",
        "\n",
        "    def __init__(self, data_root=\"cora\", rebuild=False):\n",
        "        \"\"\"Cora data, including data download, processing, loading and other functions\n",
        "        When the data cache file exists, the cache file will be used, otherwise it will be downloaded, processed, and cached to disk\n",
        "\n",
        "            The processed data can be obtained through the attribute .data, it will return a data object, including the following parts:  \n",
        "            * x: The characteristics of the node, the dimension is 2708 * 1433, the type is np.ndarray\n",
        "            * y: The label of the node, including a total of 7 categories, the type is np.ndarray\n",
        "            * adjacency: Adjacency matrix, dimension 2708 * 2708, type scipy.sparse.coo.coo_matrix\n",
        "            * train_mask: Training set mask vector, dimension is 2708, when the node belongs to the training set, the corresponding position is True, otherwise False\n",
        "            * val_mask: Verification set mask vector, dimension is 2708, when the node belongs to the verification set, the corresponding position is True, otherwise False\n",
        "            * test_mask: Test set mask vector, dimension is 2708, when the node belongs to the test set, the corresponding position is True, otherwise False\n",
        "\n",
        "        Args:\n",
        "        -------\n",
        "            data_root: string, optional\n",
        "                The directory where the data is stored, the original data path: {data_root}/raw\n",
        "                Cache data path: {data_root}/processed_cora.pkl\n",
        "            rebuild: boolean, optional\n",
        "                Whether to rebuild the data set, when set to True, the data will be reconstructed if there is cached data\n",
        "\n",
        "        \"\"\"\n",
        "        self.data_root = data_root\n",
        "        save_file = osp.join(self.data_root, \"processed_cora.pkl\")\n",
        "        if osp.exists(save_file) and not rebuild:\n",
        "            print(\"Using Cached file: {}\".format(save_file))\n",
        "            self._data = pickle.load(open(save_file, \"rb\"))\n",
        "        else:\n",
        "            self.maybe_download()\n",
        "            self._data = self.process_data()\n",
        "            with open(save_file, \"wb\") as f:\n",
        "                pickle.dump(self.data, f)\n",
        "            print(\"Cached file: {}\".format(save_file))\n",
        "    \n",
        "    @property\n",
        "    def data(self):\n",
        "        \"\"\"Return Data data objects, including x, y, adjacency, train_mask, val_mask, test_mask\"\"\"\n",
        "        return self._data\n",
        "\n",
        "    def process_data(self):\n",
        "        \"\"\"\n",
        "        Process data to get node features and labels, adjacency matrix, training set, validation set and test set\n",
        "        Quoted from：https://github.com/rusty1s/pytorch_geometric\n",
        "        \"\"\"\n",
        "        print(\"Process data ...\")\n",
        "        _, tx, allx, y, ty, ally, graph, test_index = [self.read_data(\n",
        "            osp.join(self.data_root, \"raw\", name)) for name in self.filenames]\n",
        "        train_index = np.arange(y.shape[0])\n",
        "        val_index = np.arange(y.shape[0], y.shape[0] + 500)\n",
        "        sorted_test_index = sorted(test_index)\n",
        "\n",
        "        x = np.concatenate((allx, tx), axis=0)\n",
        "        y = np.concatenate((ally, ty), axis=0).argmax(axis=1)\n",
        "\n",
        "        x[test_index] = x[sorted_test_index]\n",
        "        y[test_index] = y[sorted_test_index]\n",
        "        num_nodes = x.shape[0]\n",
        "\n",
        "        train_mask = np.zeros(num_nodes, dtype=np.bool)\n",
        "        val_mask = np.zeros(num_nodes, dtype=np.bool)\n",
        "        test_mask = np.zeros(num_nodes, dtype=np.bool)\n",
        "        train_mask[train_index] = True\n",
        "        val_mask[val_index] = True\n",
        "        test_mask[test_index] = True\n",
        "        adjacency = self.build_adjacency(graph)\n",
        "        print(\"Node's feature shape: \", x.shape)\n",
        "        print(\"Node's label shape: \", y.shape)\n",
        "        print(\"Adjacency's shape: \", adjacency.shape)\n",
        "        print(\"Number of training nodes: \", train_mask.sum())\n",
        "        print(\"Number of validation nodes: \", val_mask.sum())\n",
        "        print(\"Number of test nodes: \", test_mask.sum())\n",
        "\n",
        "        return Data(x=x, y=y, adjacency=adjacency,\n",
        "                    train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
        "\n",
        "    def maybe_download(self):\n",
        "        save_path = os.path.join(self.data_root, \"raw\")\n",
        "        for name in self.filenames:\n",
        "            if not osp.exists(osp.join(save_path, name)):\n",
        "                self.download_data(\n",
        "                    \"{}/{}\".format(self.download_url, name), save_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_adjacency(adj_dict):\n",
        "        \"\"\"Create adjacency matrix from adjacency list\"\"\"\n",
        "        edge_index = []\n",
        "        num_nodes = len(adj_dict)\n",
        "        for src, dst in adj_dict.items():\n",
        "            edge_index.extend([src, v] for v in dst)\n",
        "            edge_index.extend([v, src] for v in dst)\n",
        "        # Remove duplicate edges\n",
        "        edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
        "        edge_index = np.asarray(edge_index)\n",
        "        adjacency = sp.coo_matrix((np.ones(len(edge_index)), \n",
        "                                   (edge_index[:, 0], edge_index[:, 1])),\n",
        "                    shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
        "        return adjacency\n",
        "\n",
        "    @staticmethod\n",
        "    def read_data(path):\n",
        "        \"\"\"Use different methods to read raw data for further processing\"\"\"\n",
        "        name = osp.basename(path)\n",
        "        if name == \"ind.cora.test.index\":\n",
        "            out = np.genfromtxt(path, dtype=\"int64\")\n",
        "            return out\n",
        "        else:\n",
        "            out = pickle.load(open(path, \"rb\"), encoding=\"latin1\")\n",
        "            out = out.toarray() if hasattr(out, \"toarray\") else out\n",
        "            return out\n",
        "\n",
        "    @staticmethod\n",
        "    def download_data(url, save_path):\n",
        "        \"\"\"Data download tool, which will download when the original data does not exist\"\"\"\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        data = urllib.request.urlopen(url)\n",
        "        filename = os.path.split(url)[-1]\n",
        "\n",
        "        with open(os.path.join(save_path, filename), 'wb') as f:\n",
        "            f.write(data.read())\n",
        "\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def normalization(adjacency):\n",
        "        \"\"\"Calculation L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
        "        adjacency += sp.eye(adjacency.shape[0])    # Increase self-connection\n",
        "        degree = np.array(adjacency.sum(1))\n",
        "        d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
        "        return d_hat.dot(adjacency).dot(d_hat).tocoo()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZsOpWJph2vP"
      },
      "source": [
        "## Graph convolution layer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J279vjZ8h2vQ"
      },
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
        "        \"\"\"Graph convolution：L*X*\\theta\n",
        "\n",
        "        Args:\n",
        "        ----------\n",
        "            input_dim: int\n",
        "                Dimension of node input feature\n",
        "            output_dim: int\n",
        "                Output feature dimension\n",
        "            use_bias : bool, optional\n",
        "                Whether to use offset\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.use_bias = use_bias\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight)\n",
        "        if self.use_bias:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, adjacency, input_feature):\n",
        "        \"\"\"The adjacency matrix is a sparse matrix, so sparse matrix multiplication is used in the calculation\n",
        "    \n",
        "        Args: \n",
        "        -------\n",
        "            adjacency: torch.sparse.FloatTensor\n",
        "                Adjacency matrix\n",
        "            input_feature: torch.Tensor\n",
        "                Input characteristics\n",
        "        \"\"\"\n",
        "        support = torch.mm(input_feature, self.weight)\n",
        "        output = torch.sparse.mm(adjacency, support)\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "            + str(self.input_dim) + ' -> ' \\\n",
        "            + str(self.output_dim) + ')'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYAn51ozh2vT"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "Readers can modify and experiment the GCN model structure by themselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZKNDiygh2vV"
      },
      "source": [
        "class GcnNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a model with two layers of GraphConvolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=1433):\n",
        "        super(GcnNet, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(input_dim, 16)\n",
        "        self.gcn2 = GraphConvolution(16, 7)\n",
        "    \n",
        "    def forward(self, adjacency, feature):\n",
        "        h = F.relu(self.gcn1(adjacency, feature))\n",
        "        logits = self.gcn2(adjacency, h)\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJXrqKhh2va"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWLWmStwh2vb"
      },
      "source": [
        "# Hyperparameter definition\n",
        "LEARNING_RATE = 0.1\n",
        "WEIGHT_DACAY = 5e-4\n",
        "EPOCHS = 200\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKvOb9Mrh2vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "df087763-d138-4668-d192-7fda8b2830e8"
      },
      "source": [
        "# Load data and convert to torch.Tensor\n",
        "dataset = CoraData().data\n",
        "node_feature = dataset.x / dataset.x.sum(1, keepdims=True)  # Normalize the data so that each row is 1\n",
        "tensor_x = tensor_from_numpy(node_feature, DEVICE)\n",
        "tensor_y = tensor_from_numpy(dataset.y, DEVICE)\n",
        "tensor_train_mask = tensor_from_numpy(dataset.train_mask, DEVICE)\n",
        "tensor_val_mask = tensor_from_numpy(dataset.val_mask, DEVICE)\n",
        "tensor_test_mask = tensor_from_numpy(dataset.test_mask, DEVICE)\n",
        "normalize_adjacency = CoraData.normalization(dataset.adjacency)   # Normalized adjacency matrix\n",
        "\n",
        "num_nodes, input_dim = node_feature.shape\n",
        "indices = torch.from_numpy(np.asarray([normalize_adjacency.row, \n",
        "                                       normalize_adjacency.col]).astype('int64')).long()\n",
        "values = torch.from_numpy(normalize_adjacency.data.astype(np.float32))\n",
        "tensor_adjacency = torch.sparse.FloatTensor(indices, values, \n",
        "                                            (num_nodes, num_nodes)).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process data ...\n",
            "Node's feature shape:  (2708, 1433)\n",
            "Node's label shape:  (2708,)\n",
            "Adjacency's shape:  (2708, 2708)\n",
            "Number of training nodes:  140\n",
            "Number of validation nodes:  500\n",
            "Number of test nodes:  1000\n",
            "Cached file: cora/processed_cora.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "l9QctL7ch2vo"
      },
      "source": [
        "# Model definition: Model, Loss, Optimizer\n",
        "model = GcnNet(input_dim).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=LEARNING_RATE, \n",
        "                       weight_decay=WEIGHT_DACAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykdjB5_h2vs"
      },
      "source": [
        "# Training body function\n",
        "def train():\n",
        "    loss_history = []\n",
        "    val_acc_history = []\n",
        "    model.train()\n",
        "    train_y = tensor_y[tensor_train_mask]\n",
        "    for epoch in range(EPOCHS):\n",
        "        logits = model(tensor_adjacency, tensor_x)  # Forward propagation\n",
        "        train_mask_logits = logits[tensor_train_mask]   # Only select training nodes for supervision\n",
        "        loss = criterion(train_mask_logits, train_y)    # Calculate the loss value\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()     # Backpropagation calculation parameter gradient\n",
        "        optimizer.step()    # Gradient update using optimization method\n",
        "        train_acc, _, _ = test(tensor_train_mask)     # Calculate the accuracy on the current model training set\n",
        "        val_acc, _, _ = test(tensor_val_mask)     # Calculate the accuracy of the current model on the validation set\n",
        "        # Record the change of loss value and accuracy during training, used for drawing\n",
        "        loss_history.append(loss.item())\n",
        "        val_acc_history.append(val_acc.item())\n",
        "        print(\"Epoch {:03d}: Loss {:.4f}, TrainAcc {:.4}, ValAcc {:.4f}\".format(\n",
        "            epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
        "    \n",
        "    return loss_history, val_acc_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJFzg3qh2vv"
      },
      "source": [
        "# Test function\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor_adjacency, tensor_x)\n",
        "        test_mask_logits = logits[mask]\n",
        "        predict_y = test_mask_logits.max(1)[1]\n",
        "        accuarcy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
        "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_y[mask].cpu().numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iWcnctZh2vy"
      },
      "source": [
        "def plot_loss_with_acc(loss_history, val_acc_history):\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    ax1.plot(range(len(loss_history)), loss_history,\n",
        "             c=np.array([255, 71, 90]) / 255.)\n",
        "    plt.ylabel('Loss')\n",
        "    \n",
        "    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n",
        "    ax2.plot(range(len(val_acc_history)), val_acc_history,\n",
        "             c=np.array([79, 179, 255]) / 255.)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.yaxis.set_label_position(\"right\")\n",
        "    plt.ylabel('ValAcc')\n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title('Training Loss & Validation Accuracy')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRtST-w-h2v2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35a75678-7673-461f-c3cb-8f633930b222"
      },
      "source": [
        "loss, val_acc = train()\n",
        "test_acc, test_logits, test_label = test(tensor_test_mask)\n",
        "print(\"Test accuarcy: \", test_acc.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss 1.9505, TrainAcc 0.35, ValAcc 0.2040\n",
            "Epoch 001: Loss 1.8636, TrainAcc 0.4429, ValAcc 0.2680\n",
            "Epoch 002: Loss 1.7784, TrainAcc 0.7643, ValAcc 0.5460\n",
            "Epoch 003: Loss 1.6486, TrainAcc 0.8857, ValAcc 0.6820\n",
            "Epoch 004: Loss 1.4973, TrainAcc 0.9214, ValAcc 0.6880\n",
            "Epoch 005: Loss 1.3349, TrainAcc 0.9143, ValAcc 0.7040\n",
            "Epoch 006: Loss 1.1708, TrainAcc 0.9357, ValAcc 0.7180\n",
            "Epoch 007: Loss 1.0099, TrainAcc 0.9429, ValAcc 0.7300\n",
            "Epoch 008: Loss 0.8592, TrainAcc 0.9643, ValAcc 0.7440\n",
            "Epoch 009: Loss 0.7256, TrainAcc 0.9643, ValAcc 0.7620\n",
            "Epoch 010: Loss 0.6099, TrainAcc 0.9643, ValAcc 0.7720\n",
            "Epoch 011: Loss 0.5140, TrainAcc 0.9714, ValAcc 0.7680\n",
            "Epoch 012: Loss 0.4367, TrainAcc 0.9714, ValAcc 0.7700\n",
            "Epoch 013: Loss 0.3757, TrainAcc 0.9857, ValAcc 0.7840\n",
            "Epoch 014: Loss 0.3277, TrainAcc 0.9857, ValAcc 0.7800\n",
            "Epoch 015: Loss 0.2912, TrainAcc 0.9929, ValAcc 0.7760\n",
            "Epoch 016: Loss 0.2641, TrainAcc 0.9929, ValAcc 0.7820\n",
            "Epoch 017: Loss 0.2439, TrainAcc 0.9929, ValAcc 0.7840\n",
            "Epoch 018: Loss 0.2300, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 019: Loss 0.2207, TrainAcc 1.0, ValAcc 0.7840\n",
            "Epoch 020: Loss 0.2145, TrainAcc 1.0, ValAcc 0.7780\n",
            "Epoch 021: Loss 0.2106, TrainAcc 1.0, ValAcc 0.7780\n",
            "Epoch 022: Loss 0.2077, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 023: Loss 0.2051, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 024: Loss 0.2020, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 025: Loss 0.1979, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 026: Loss 0.1927, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 027: Loss 0.1863, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 028: Loss 0.1793, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 029: Loss 0.1722, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 030: Loss 0.1651, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 031: Loss 0.1587, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 032: Loss 0.1530, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 033: Loss 0.1483, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 034: Loss 0.1448, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 035: Loss 0.1426, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 036: Loss 0.1416, TrainAcc 1.0, ValAcc 0.7840\n",
            "Epoch 037: Loss 0.1392, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 038: Loss 0.1341, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 039: Loss 0.1319, TrainAcc 1.0, ValAcc 0.7820\n",
            "Epoch 040: Loss 0.1326, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 041: Loss 0.1311, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 042: Loss 0.1284, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 043: Loss 0.1277, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 044: Loss 0.1278, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 045: Loss 0.1261, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 046: Loss 0.1245, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 047: Loss 0.1242, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 048: Loss 0.1238, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 049: Loss 0.1221, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 050: Loss 0.1209, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 051: Loss 0.1211, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 052: Loss 0.1206, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 053: Loss 0.1192, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 054: Loss 0.1188, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 055: Loss 0.1189, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 056: Loss 0.1183, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 057: Loss 0.1179, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 058: Loss 0.1174, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 059: Loss 0.1173, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 060: Loss 0.1174, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 061: Loss 0.1166, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 062: Loss 0.1156, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 063: Loss 0.1154, TrainAcc 1.0, ValAcc 0.7800\n",
            "Epoch 064: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 065: Loss 0.1143, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 066: Loss 0.1139, TrainAcc 1.0, ValAcc 0.7780\n",
            "Epoch 067: Loss 0.1143, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 068: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 069: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 070: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 071: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 072: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 073: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 074: Loss 0.1139, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 075: Loss 0.1135, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 076: Loss 0.1135, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 077: Loss 0.1132, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 078: Loss 0.1132, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 079: Loss 0.1138, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 080: Loss 0.1143, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 081: Loss 0.1147, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 082: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 083: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 084: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 085: Loss 0.1147, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 086: Loss 0.1139, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 087: Loss 0.1129, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 088: Loss 0.1127, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 089: Loss 0.1134, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 090: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 091: Loss 0.1145, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 092: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 093: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 094: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 095: Loss 0.1144, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 096: Loss 0.1138, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 097: Loss 0.1134, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 098: Loss 0.1136, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 099: Loss 0.1142, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 100: Loss 0.1145, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 101: Loss 0.1147, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 102: Loss 0.1147, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 103: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 104: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 105: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 106: Loss 0.1152, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 107: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 108: Loss 0.1146, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 109: Loss 0.1140, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 110: Loss 0.1138, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 111: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 112: Loss 0.1145, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 113: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 114: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 115: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 116: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 117: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 118: Loss 0.1155, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 119: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 120: Loss 0.1159, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 121: Loss 0.1155, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 122: Loss 0.1147, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 123: Loss 0.1138, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 124: Loss 0.1137, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 125: Loss 0.1145, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 126: Loss 0.1153, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 127: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 128: Loss 0.1156, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 129: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 130: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 131: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 132: Loss 0.1157, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 133: Loss 0.1160, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 134: Loss 0.1159, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 135: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 136: Loss 0.1144, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 137: Loss 0.1143, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 138: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 139: Loss 0.1152, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 140: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 141: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 142: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 143: Loss 0.1154, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 144: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 145: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 146: Loss 0.1156, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 147: Loss 0.1151, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 148: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 149: Loss 0.1152, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 150: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 151: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 152: Loss 0.1145, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 153: Loss 0.1144, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 154: Loss 0.1148, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 155: Loss 0.1153, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 156: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 157: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 158: Loss 0.1157, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 159: Loss 0.1155, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 160: Loss 0.1154, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 161: Loss 0.1155, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 162: Loss 0.1154, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 163: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 164: Loss 0.1143, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 165: Loss 0.1140, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 166: Loss 0.1144, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 167: Loss 0.1151, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 168: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 169: Loss 0.1163, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 170: Loss 0.1164, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 171: Loss 0.1162, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 172: Loss 0.1158, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 173: Loss 0.1152, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 174: Loss 0.1144, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 175: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 176: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 177: Loss 0.1146, TrainAcc 1.0, ValAcc 0.7940\n",
            "Epoch 178: Loss 0.1153, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 179: Loss 0.1160, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 180: Loss 0.1164, TrainAcc 1.0, ValAcc 0.8040\n",
            "Epoch 181: Loss 0.1163, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 182: Loss 0.1159, TrainAcc 1.0, ValAcc 0.7960\n",
            "Epoch 183: Loss 0.1150, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 184: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 185: Loss 0.1137, TrainAcc 1.0, ValAcc 0.7980\n",
            "Epoch 186: Loss 0.1141, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 187: Loss 0.1151, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 188: Loss 0.1160, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 189: Loss 0.1164, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 190: Loss 0.1160, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 191: Loss 0.1152, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 192: Loss 0.1142, TrainAcc 1.0, ValAcc 0.7920\n",
            "Epoch 193: Loss 0.1138, TrainAcc 1.0, ValAcc 0.7900\n",
            "Epoch 194: Loss 0.1142, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 195: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7860\n",
            "Epoch 196: Loss 0.1155, TrainAcc 1.0, ValAcc 0.8020\n",
            "Epoch 197: Loss 0.1157, TrainAcc 1.0, ValAcc 0.7880\n",
            "Epoch 198: Loss 0.1155, TrainAcc 1.0, ValAcc 0.8000\n",
            "Epoch 199: Loss 0.1149, TrainAcc 1.0, ValAcc 0.7960\n",
            "Test accuarcy:  0.8109999895095825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbPR93mLRhWQ"
      },
      "source": [
        "plot_loss_with_acc(loss, val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpHn-BXRlAN"
      },
      "source": [
        "# Draw TSNE dimension reduction graph of test data\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE()\n",
        "out = tsne.fit_transform(test_logits)\n",
        "fig = plt.figure()\n",
        "for i in range(7):\n",
        "    indices = test_label == i\n",
        "    x, y = out[indices].T\n",
        "    plt.scatter(x, y, label=str(i))\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}